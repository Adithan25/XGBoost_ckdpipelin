{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting chronic kidney disease using an XGBoost pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook disaplys a use case of utilizing XGBoost in a pipeline to process and predict chronic kidney disease in patients. The <a href=\"https://archive.ics.uci.edu/ml/datasets/chronic_kidney_disease\"> dataset</a> is provided by the UCI Machine Learning repository.\n",
    "\n",
    "The first step is to load all necessary modules and read in the dataset. The names of the columns is not included in the dataset so we will name them when loading using the description given on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('chronic_kidney_disease.csv',\n",
    "               header=None,names=['age','bp','sg','al','su','rbc','pc','pcc','ba','bgr','bu','sc',\n",
    "                                  'sod','pot','hemo','pcv','wc','rc','htn','dm','cad','appet','pe',\n",
    "                                  'ane','class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  age  bp     sg al su     rbc        pc         pcc          ba  bgr  ...  \\\n",
      "0  48  80  1.020  1  0       ?    normal  notpresent  notpresent  121  ...   \n",
      "1   7  50  1.020  4  0       ?    normal  notpresent  notpresent    ?  ...   \n",
      "2  62  80  1.010  2  3  normal    normal  notpresent  notpresent  423  ...   \n",
      "3  48  70  1.005  4  0  normal  abnormal     present  notpresent  117  ...   \n",
      "4  51  80  1.010  2  0  normal    normal  notpresent  notpresent  106  ...   \n",
      "\n",
      "  pcv    wc   rc  htn   dm cad appet   pe  ane class  \n",
      "0  44  7800  5.2  yes  yes  no  good   no   no   ckd  \n",
      "1  38  6000    ?   no   no  no  good   no   no   ckd  \n",
      "2  31  7500    ?   no  yes  no  poor   no  yes   ckd  \n",
      "3  32  6700  3.9  yes   no  no  poor  yes  yes   ckd  \n",
      "4  35  7300  4.6   no   no  no  good   no   no   ckd  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "age      object\n",
      "bp       object\n",
      "sg       object\n",
      "al       object\n",
      "su       object\n",
      "rbc      object\n",
      "pc       object\n",
      "pcc      object\n",
      "ba       object\n",
      "bgr      object\n",
      "bu       object\n",
      "sc       object\n",
      "sod      object\n",
      "pot      object\n",
      "hemo     object\n",
      "pcv      object\n",
      "wc       object\n",
      "rc       object\n",
      "htn      object\n",
      "dm       object\n",
      "cad      object\n",
      "appet    object\n",
      "pe       object\n",
      "ane      object\n",
      "class    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Upon initial inspection, we see null values are displayes as '?' and all the columns are of object type. This is not ideal, I will first replace '?' with NaNs which will make it easier to process later. I will also convert columns with numeric values to float type, the description of each column is given in the , I use it to inform which columns should be numerical and categorical. The column names of each column type will be listed in 'numeric_cols' and 'cat_cols'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('?',np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols=['age','bp','sg','al','su','bgr','bu','sc','sod','pot','hemo','pcv','wc','rc']\n",
    "cat_cols=['rbc', 'pc', 'pcc', 'ba', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age    bp     sg   al   su     rbc        pc         pcc          ba  \\\n",
      "0  48.0  80.0  1.020  1.0  0.0     NaN    normal  notpresent  notpresent   \n",
      "1   7.0  50.0  1.020  4.0  0.0     NaN    normal  notpresent  notpresent   \n",
      "2  62.0  80.0  1.010  2.0  3.0  normal    normal  notpresent  notpresent   \n",
      "3  48.0  70.0  1.005  4.0  0.0  normal  abnormal     present  notpresent   \n",
      "4  51.0  80.0  1.010  2.0  0.0  normal    normal  notpresent  notpresent   \n",
      "\n",
      "     bgr  ...   pcv      wc   rc  htn   dm  cad  appet   pe  ane class  \n",
      "0  121.0  ...  44.0  7800.0  5.2  yes  yes   no   good   no   no   ckd  \n",
      "1    NaN  ...  38.0  6000.0  NaN   no   no   no   good   no   no   ckd  \n",
      "2  423.0  ...  31.0  7500.0  NaN   no  yes   no   poor   no  yes   ckd  \n",
      "3  117.0  ...  32.0  6700.0  3.9  yes   no   no   poor  yes  yes   ckd  \n",
      "4  106.0  ...  35.0  7300.0  4.6   no   no   no   good   no   no   ckd  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "age      float64\n",
      "bp       float64\n",
      "sg       float64\n",
      "al       float64\n",
      "su       float64\n",
      "rbc       object\n",
      "pc        object\n",
      "pcc       object\n",
      "ba        object\n",
      "bgr      float64\n",
      "bu       float64\n",
      "sc       float64\n",
      "sod      float64\n",
      "pot      float64\n",
      "hemo     float64\n",
      "pcv      float64\n",
      "wc       float64\n",
      "rc       float64\n",
      "htn       object\n",
      "dm        object\n",
      "cad       object\n",
      "appet     object\n",
      "pe        object\n",
      "ane       object\n",
      "class     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is in an easily processible form, I will separate the target labels ('class' column) and feature table without target labels. The target labels will be converted to 0 for no kidney disease and 1 for a positive case of chronic kidney disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target=df['class'].to_numpy()\n",
    "y=np.where(target=='ckd',1,0)\n",
    "feature=df.drop(['class'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I begin constructing the pipeline steps. Since there are missing values, we will use an imputer to substitute null values with median values of the column in the case of numeric variables, and most frequent value of the column in the case of categorical variables. Then we need to encode the categorical variables in order to feed it into the ML model, for this I will use a onehotencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = SimpleImputer(strategy='median')\n",
    "\n",
    "\n",
    "categoric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the two imputers need to be combined and the numeric_transfomer needs to be applied on the numeric_cols and categoric_transfomer to cat_cols. I use a ColumnTransformer to combine these two imputers which will enable me to include it as a pipeline step in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categoric_transformer, cat_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I construct the main pipeline, first containing the 'preprocessor' step and an 'xgbclf' step which contains the XGBoost classifier. The initial classifier will have trees with a max depth of 3, use the 'logloss' metric which is an appropriate loss function for a binary classification problem. The model will run parellely using 3 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('xgbclf', xgb.XGBClassifier(max_depth=3,eval_metric='logloss',n_jobs=3,use_label_encoder=False))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure the pipeline has  no errors and the data is in correct format, I apply the pipeline with cross validation using 5 folds. The model will be evaluated using the ROC AUC score since we want the model to ideally classify positive and negative cases correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_scores = cross_val_score(pipeline, feature, y, scoring=\"roc_auc\", cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9982666666666666\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean ROC AUC score is 0.999 which is extremely good for an untuned model! This model can be used as is without further tuning depending on requirements of the problem. However, for demonstration purposes, I will see if the model can be improved by tuning the hyperparameters. The three hyperparameters tuned here will be the learning rate, max depth of each tree and number of trees or 'boosters' used. The complete list of hyperparameters can be found in the <a href=\"https://archive.ics.uci.edu/ml/datasets/chronic_kidney_disease\"> XGB documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid = {\n",
    "    'xgbclf__learning_rate': np.arange(0.05, 1, 0.05),\n",
    "    'xgbclf__max_depth': np.arange(3,15, 1),\n",
    "    'xgbclf__n_estimators': np.arange(50, 250, 50)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use a randomized grid search to sample 100 different parameters settings from the grid above. For each parameter setting, the model will be cross validated with 4 folds, this will produce 100 $\\times$ 4 = 400 total fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_roc_auc = RandomizedSearchCV(pipeline,param_distributions=xgb_grid,\n",
    "                                        n_iter=100,scoring='roc_auc',verbose=1,cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 100 candidates, totalling 400 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=4,\n",
       "                   estimator=Pipeline(steps=[('preprocessor',\n",
       "                                              ColumnTransformer(transformers=[('num',\n",
       "                                                                               SimpleImputer(strategy='median'),\n",
       "                                                                               ['age',\n",
       "                                                                                'bp',\n",
       "                                                                                'sg',\n",
       "                                                                                'al',\n",
       "                                                                                'su',\n",
       "                                                                                'bgr',\n",
       "                                                                                'bu',\n",
       "                                                                                'sc',\n",
       "                                                                                'sod',\n",
       "                                                                                'pot',\n",
       "                                                                                'hemo',\n",
       "                                                                                'pcv',\n",
       "                                                                                'wc',\n",
       "                                                                                'rc']),\n",
       "                                                                              ('cat',\n",
       "                                                                               Pipeline(steps=[('imputer',\n",
       "                                                                                                SimpleImputer(strategy='most_frequent')),\n",
       "                                                                                               ('onehot',\n",
       "                                                                                                OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                                               ['rbc...\n",
       "                                                            use_label_encoder=False,\n",
       "                                                            validate_parameters=None,\n",
       "                                                            verbosity=None))]),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={'xgbclf__learning_rate': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95]),\n",
       "                                        'xgbclf__max_depth': array([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n",
       "                                        'xgbclf__n_estimators': array([ 50, 100, 150, 200])},\n",
       "                   scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomized_roc_auc.fit(feature,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997854997854998\n",
      "{'xgbclf__n_estimators': 50, 'xgbclf__max_depth': 14, 'xgbclf__learning_rate': 0.6500000000000001}\n"
     ]
    }
   ],
   "source": [
    "print(randomized_roc_auc.best_score_)\n",
    "print(randomized_roc_auc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The tuned model produces a slightly higher ROC AUC score and the hyperparameter values for the best model is displayed above. This model can now be tuned further if desired of validated on a test set before sending to production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
